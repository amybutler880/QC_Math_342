Title: Math 342W Final Project
Author: Amy Butler
Output: PDF Document
Date: May 25, 2021

Upload the necessary libraries:
```{r}
pacman::p_load(dplyr)
pacman::p_load(tidyverse)
pacman::p_load(missForest)
pacman::p_load(rpart)
pacman::p_load(rpart.plot)
pacman::p_load(Metrics)
pacman::p_load(randomForest)
pacman::p_load(caret)
pacman::p_load(mlr)
pacman::p_load(magrittr)
```

Import the csv data file:
```{r}
HousingData = read.csv("C:/Users/aliza/Desktop/housing_data_2016_2017.csv", header=TRUE)
```

Information about the data:
```{r}
#Number of sample homes:
nrow(HousingData)

#Number of description variables:
ncol(HousingData)

#Number of unique zip codes: 55 - As counted in the chart provided.

#Number of zip code factors after grouping by region: 9 - As counted in the chart provided.

#Number of zip codes in all of Queens: 79 - As reported by Zip-Codes.com

#Percentage of available data being represented: Approximately 70%
(55/79)*100
```

Remove useless columns that have no meaning, contain the same value in every row, or contains all NA's:
```{r}
HousingData %<>%
  select(-c(HITId, HITTypeId, Title, Description, Keywords, Reward, CreationTime, MaxAssignments, RequesterAnnotation,
            AssignmentDurationInSeconds, AutoApprovalDelayInSeconds, Expiration, NumberOfSimilarHITs, LifetimeInSeconds, 
            AssignmentId, WorkerId, AssignmentStatus,AcceptTime, SubmitTime, AutoApprovalTime, ApprovalTime, RejectionTime, 
            RequesterFeedback, WorkTimeInSeconds, LifetimeApprovalRate, Last30DaysApprovalRate, Last7DaysApprovalRate, 
            date_of_sale, model_type, full_address_or_zip_code, listing_price_to_nearest_1000,url, community_district_num))
```

Create numeric features by removing currency symbols:
```{r}
#Common_Charges
HousingData$common_charges=as.numeric(gsub("[$,]","",HousingData$common_charges))

#Maintenance_Cost
HousingData$maintenance_cost=as.numeric(gsub("[$,]","",HousingData$maintenance_cost))

#Parking_Charges
HousingData$parking_charges=as.numeric(gsub("[$,]","",HousingData$parking_charges))

#Sale_Price
HousingData$sale_price=as.numeric(gsub("[$,]","",HousingData$sale_price))

#Total_Taxes
HousingData$total_taxes=as.numeric(gsub("[$,]","",HousingData$total_taxes))
```

Remove the rows that don't have a sales price:
```{r}
HousingData = HousingData[!is.na(HousingData$sale_price), ]
```

Create a new column of the zip codes:
```{r}
HousingData$zip_code=as.numeric(str_extract(sapply(HousingData$URL,substring,45,150),"\\d{5}"))
```

Drop the URL column:
```{r}
HousingData$URL = NULL
```

Create a new column called area and group the zip codes based on area:
```{r}
zip=HousingData$zip_code
HousingData = HousingData %>%
  mutate(area = as.factor(
      ifelse(zip>=11361 & zip<=11364, "Northeast Queens",
      ifelse(zip>=11354 & zip<=11360, "North Queens",
      ifelse(zip>=11365 & zip<=11367, "Central Queens",
      ifelse(zip==11436 | zip==11423 | (zip>=11432 & zip<=11436), "Jamaica",
      ifelse(zip>=11101 & zip<=11106, "Northwest Queens",
      ifelse(zip==11374 | zip==11375 | zip==11379 | zip==11385, "West Central Queens", 
      ifelse(zip==11004 | zip==11005 | zip==11411 | zip==11422 | (zip>=11426 & zip<=11429), "Southest Queens",
      ifelse(zip>=11413 & zip<=11421, "Southwest Queens", 
      ifelse(zip==11368 | zip==11369 | zip==11370 | zip==11372 | zip==11373 | zip==11377 | zip==11378, "West Queens", NA)))))))))))
```

Remove features that are missing more than 70% of it's observations:
```{r}
colMeans(is.na(HousingData))

HousingData$parking_charges = NULL
HousingData$pct_tax_deductibl = NULL
HousingData$garage_exists = NULL
HousingData$num_half_bathrooms = NULL
HousingData$common_charges = NULL
HousingData$total_taxes = NULL
```
Remove the zip code column:
```{r}
HousingData$zip_code = NULL
```

Binarize the coop_condo feature:
```{r}
HousingData = HousingData %>%
  mutate(coop_condo = ifelse(coop_condo == "condo",0,1))
```

Binarize the dogs_allowed feature:
```{r}
HousingData = HousingData %>%
  mutate(dogs_allowed = ifelse(dogs_allowed == "yes",1,0))
```

Binarize the cats_allowed feature:
```{r}
HousingData = HousingData %>%
  mutate(cats_allowed = ifelse(cats_allowed == "yes",1,0))
```

Change all string features to lowercase:
```{r}
HousingData %<>%
  mutate_at(c("dining_room_type", "fuel_type", "kitchen_type"), tolower)
```

Factorize the kitchen_type feature:
```{r}
HousingData$kitchen_type = factor(HousingData$kitchen_type)

#Count how many of each type of kitchen are in the feature:
table(HousingData$kitchen_type)
```

Fix inconsistency in kitchen type:
```{r}
#Replace kitchen type "1955" with "efficiency because it is the most likely":
HousingData[330,7]="efficiency"

HousingData$kitchen_type = factor(HousingData$kitchen_type)

#Count how many of each type of kitchen are in the feature:
table(HousingData$kitchen_type)
```

Factorize the dining_room_type feature:
```{r}
HousingData$dining_room_type= factor(HousingData$dining_room_type)
levels(HousingData$dining_room_type)
```

Factorize the fuel_type feature:
```{r}
HousingData$fuel_type= factor(HousingData$fuel_type)
levels(HousingData$fuel_type)
```
Group years as decades:
```{r}
HousingData = HousingData %>%
    mutate(decade = as.factor(
      ifelse(approx_year_built>=1910 & approx_year_built<1920, "1910",
      ifelse(approx_year_built>=1920 & approx_year_built<1930, "1920",
      ifelse(approx_year_built>=1930 & approx_year_built<1940, "1930",
      ifelse(approx_year_built>=1940 & approx_year_built<1950, "1940",
      ifelse(approx_year_built>=1950 & approx_year_built<1960, "1950",
      ifelse(approx_year_built>=1960 & approx_year_built<1970, "1960",
      ifelse(approx_year_built>=1970 & approx_year_built<1980, "1970",
      ifelse(approx_year_built>=1980 & approx_year_built<1990, "1980",
      ifelse(approx_year_built>=1990 & approx_year_built<2000, "1990",
      ifelse(approx_year_built>=2000 & approx_year_built<2010, "2000",
      ifelse(approx_year_built>=2010 & approx_year_built<2020, "2010", NA )))))))))))))
```

Drop the year column:
```{r}
HousingData$approx_year_built=NULL
```

Find which variables are continuous and which are nominal:
```{r}
str(HousingData)
```

Summarize all of the continuous variables:
```{r}
library("psych")
describe(HousingData$num_bedrooms)
describe(HousingData$num_floors_in_building)
describe(HousingData$num_full_bathrooms)
describe(HousingData$num_total_rooms)
describe(HousingData$sq_footage)
describe(HousingData$walk_score)
describe(HousingData$maintenance_cost)
describe(HousingData$sale_price)
```

Summarize the nominal variables:
```{r}
#cats_allowed
cats=table(HousingData$cats_allowed)
addmargins(cats)
round(prop.table(cats),digits=2)
```

```{r}
#coop_condo
cc=table(HousingData$coop_condo)
addmargins(cc)
round(prop.table(cc),digits=2)
```

```{r}
#dining_room_type
drt=table(HousingData$dining_room_type)
addmargins(drt)
round(prop.table(drt),digits=3)
```

```{r}
#dogs_allowed
dogs=table(HousingData$dogs_allowed)
addmargins(dogs)
round(prop.table(dogs),digits=2)
```

```{r}
#fuel_type
fuel=table(HousingData$fuel_type)
addmargins(fuel)
round(prop.table(fuel),digits=2)
```

```{r}
#kitchen_type
kitchen=table(HousingData$kitchen_type)
addmargins(kitchen)
round(prop.table(kitchen),digits=2)
```

```{r}
#area
area=table(HousingData$area)
addmargins(area)
round(prop.table(area),digits=4)
```

```{r}
#decade
decade=table(HousingData$decade)
addmargins(decade)
round(prop.table(decade),digits=4)
```

Impute the missing data:
```{r}
#y is sale_price
y = HousingData$sale_price

#X is the data without sale_price
X = HousingData %>%
  select(-sale_price)

#Create a matrix with p columns that represents missingness
M=tbl_df(apply(is.na(X),2,as.numeric))
colnames(M) = paste("is_missing_", colnames(X), sep = "")
M=tbl_df(t(unique(t(M))))
M %<>% 
  select_if(function(x){sum(x)>0})

#Impute using MissForest:
Ximp=missForest(data.frame(X), sampsize=rep(200, ncol(X)))$Ximp
Ximp = missForest(data.frame(X), sampsize = rep(200, ncol(X)))$ximp
Ximp_and_missing_dummies = data.frame(cbind(Ximp, M))
newdata = cbind(Ximp_and_missing_dummies, y)
newdata %<>%
  rename(sale_price = y) %<>%
  select(sale_price, everything())
X = newdata[ ,2:ncol(newdata)]
y = newdata[ ,1]

#Set the new HousingData with imputed values:
HousingData=newdata
```

Tree Regression:
```{r}
#Regression tree with training data:
tree_model=rpart(lm(HousingData$sale_price ~ . ,data=HousingData), method="anova", cp=0.0001, maxdepth=6)
bestcp = tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"]
tree_model.pruned = prune(tree_model, cp = bestcp)
rpart.plot(tree_model.pruned, box.palette="RdBu", shadow.col="gray", nn=TRUE, faclen=3, cex=0.7, fallen.leaves=FALSE)
text(tree_model.pruned)
prp(tree_model.pruned, faclen = 3, cex = 0.6, extra = 1)
```

Split the data into a training set and test set:
```{r}
Split = sort(sample(nrow(HousingData),nrow(HousingData)*.7))

#Training Set:
train = HousingData[Split,]
y_train = train$sale_price
X_train = subset(train, select=-c(sale_price))

#Test Set
test = HousingData[-Split,]
y_test = test$sale_price
X_test = subset(test, select=-c(sale_price))
```

Fit a linear model with the training set:
```{r}
OLS_model = lm(y_train ~ . ,data=X_train)
summary(OLS_model)
```

Predict the linear model on the test set:
```{r}
predictions = predict(OLS_model, test)
summary(predictions)
```

Create a dataframe of the actual and predicted values:
```{r}
actual_predict = data.frame(cbind(actuals=test$sale_price, predicted=predictions))
head(actual_predict)
```

Find MSE & RMSE:
```{r}
actual_predict$Square_Error=(actual_predict$actuals-actual_predict$predicted)^2
print(actual_predict)

MSE=sum(actual_predict$Square_Error)/nrow(actual_predict)
print(MSE)

RMSE=sqrt(MSE)
print(RMSE)
```

Create the default random forest model:
```{r}
rf_model=randomForest(sale_price ~., data=train)
print(rf_model)
attributes(rf_model)
```

Prediction using the default random forest model:
```{r}
rf_pred = predict(rf_model, test)
head(rf_pred)
```

Plot the error rate of the default random forest model:
```{r}
plot(rf_model)
```

Find the ideal mTry value:
```{r}
#Find the ideal mTry:
tune_rf= tuneRF(X_train,
                y_train,
                stepFactor=0.5,
                plot=TRUE,
                ntreeTry=300,
                trace=TRUE,
                improve = 0.05)

print(tune_rf)
```

Find the ideal node size value:
```{r}
#Histogram of tree size in terms of number of nodes:
hist(treesize(rf_model), main ="Number of Nodes for the Trees", col="blue")
```

Find out which variables are important in the default model:
```{r}
varImpPlot(rf_model, sort =TRUE, main = "Default Variable Importance")
```

Find how often each variable is used in the default random forest model by level of importance:
```{r}
varUsed(rf_model)
```

Tune the default random forest model:
```{r}
#Tune the default model:
tuned_rf_model = randomForest(sale_price ~., 
                              data=train,
                              ntree = 300,
                              mtry = 7,
                              importance = TRUE)
                          
#Print the tuned model:
print(tuned_rf_model)
```

Prediction using the tuned random forest model:
```{r}
tuned_rf_predicted = predict(tuned_rf_model, test)
head(tuned_rf_predicted)
```

Find out which variables are important in the tuned model:
```{r}
varImpPlot(tuned_rf_model, sort =TRUE, main = "Tuned Variable Importance")
```

Find how often each variable is used in the default random forest model by level of importance:
```{r}
varUsed(tuned_rf_model)
```

Find the average in sample RMSE and R^2:
```{r}
set.seed(222)

#Default Model:
drf=randomForest(sale_price ~., data=train)
Default_RMSE = mean(sqrt(drf$mse))
Default_Rsq = mean(drf$rsq)
Default_RMSE
Default_Rsq

#Tuned Model:
trf = randomForest(sale_price ~., 
                   data = train,
                   ntree= 300,
                   mtry=7,
                   importance=TRUE)
Tuned_RMSE =mean(sqrt(trf$mse))
Tuned_Rsq = mean(trf$rsq)
Tuned_RMSE
Tuned_Rsq

#Create a table:
is_metrics = matrix(c(Default_RMSE, Default_Rsq, Tuned_RMSE, Tuned_Rsq), ncol=2, byrow = TRUE)
colnames(is_metrics)=c("RMSE", "R_Squared")
rownames(is_metrics)=c("Default Model", "Tuned Model")
is_metrics = as.table(is_metrics)
is_metrics
```

Find the average oob RMSE and R^2:
```{r}
set.seed(223)

#Default Model:
drf=randomForest(sale_price ~., data=test)
Default_RMSE = mean(sqrt(drf$mse))
Default_Rsq = mean(drf$rsq)
Default_RMSE
Default_Rsq

#Tuned Model:
trf = randomForest(sale_price ~., 
                   data = test,
                   ntree= 300,
                   mtry=7,
                   importance=TRUE)
Tuned_RMSE =mean(sqrt(trf$mse))
Tuned_Rsq = mean(trf$rsq)
Tuned_RMSE
Tuned_Rsq

#Create a table:
oob_metrics =matrix(c(Default_RMSE, Default_Rsq, Tuned_RMSE, Tuned_Rsq), ncol=2, byrow = TRUE)
colnames(oob_metrics)=c("RMSE", "R_Squared")
rownames(oob_metrics)=c("Default Model", "Tuned Model")
oob_metrics = as.table(oob_metrics)
oob_metrics
```