---
title: "Lab 5"
author: "Amy Butler"
date: "11:59PM March 18, 2021"
output:
  word_document: default
  pdf_document: default
---

#Create a 2x2 matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns.

```{r}
norm_vec=function(v){
  sqrt(sum(v^2))
}
X=matrix(1:1,nrow=2,ncol=2)
X[,2]=rnorm(2)
cos_theta= t(X[,1]%*%X[,2]) / (norm_vec(X[,1])*norm_vec(X[,2]))
cos_theta
abs(90-acos(cos_theta)*180/pi)
```

#Repeat this exercise `Nsim = 1e5` times and report the average absolute angle.

```{r}
Nsim = 1e5
angles=array(NA,Nsim)
for (i in 1:Nsim){
  X=matrix(1:1,nrow=2,ncol=2)
  X[,2]=rnorm(2)
  cos_theta= t(X[,1]%*%X[,2]) / (norm_vec(X[,1])*norm_vec(X[,2]))
  angles[i]=abs(90-acos(cos_theta)*180/pi)
}
mean(angles)
```

#Create a 2xn matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns. For n = 10, 50, 100, 200, 500, 1000, report the average absolute angle over `Nsim = 1e5` simulations.

```{r}
N_s=c(2,5,10, 50, 100, 200, 500, 1000)
Nsim = 1e5
angles=matrix(NA,nrow=Nsim,ncol=length(N_s))
for (j in 1:length(N_s)){
  for (i in 1:Nsim){
    X=matrix(1,nrow=N_s[j],ncol=2)
    X[,2]=rnorm(N_s[j])
    cos_theta= t(X[,1]%*%X[,2]) / (norm_vec(X[,1])*norm_vec(X[,2]))
    angles[i,j]=abs(90-acos(cos_theta)*180/pi)
  }
}
colMeans(angles)
```

#What is this absolute angle converging to? Why does this make sense?
  
The absolute angle difference from ninety is converging to zero. This makes sense because in a high dimensional space random directions are orthogonal.
  
#Create a vector y by simulating n = 100 standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the R^2 of an OLS regression of `y ~ X`. Use matrix algebra.

```{r}
n=100
X=cbind(1,rnorm(n))
y=rnorm(n)

H=X %*% solve((t(X)%*% X)) %*% t(X)
y_hat= H%*% y
y_bar= mean(y)

SSR=sum((y_hat-y_bar)^2)
SST=sum((y-y_bar)^2)

Rsq=(SSR/SST)
Rsq
```

#Write a for loop to each time bind a new column of 100 standard iid normals to the matrix X and find the R^2 each time until the number of columns is 100. Create a vector to save all R^2's. What happened??

```{r}
Rsq_s=array(NA,dim=n-2)
for (j in 1:(n-2)){
  X= cbind(X,rnorm(n))
  H=X %*% solve((t(X) %*% X)) %*% t(X)
  y_hat= H %*% y
  y_bar= mean(y)
  SSR=sum((y_hat-y_bar)^2)
  SST=sum((y-y_bar)^2)
  Rsq_s[j]=(SSR/SST)
}
Rsq_s
diff(Rsq_s)
```

#Test that the projection matrix onto this X is the same as I_n. You may have to vectorize the matrices in the `expect_equal` function for the test to work.

```{r}
pacman::p_load(testthat)
#dim(X)
#H=X %*% solve((t(X)%*% X)) %*% t(X)
#H[1:10,1:10]
I=diag(n)
expect_equal(H,I)
```

#Add one final column to X to bring the number of columns to 101. Then try to compute R^2. What happens? 


X= cbind(X,rnorm(n))
H=X %*% solve((t(X) %*% X)) %*% t(X)
y_hat= H %*% y
y_bar= mean(y)
SSR=sum((y_hat-y_bar)^2)
SST=sum((y-y_bar)^2)
Rsq=SSR/SST


#Why does this make sense?

It fails because X-transpose-X is rank deficient making it impossible to invert.

#Write a function spec'd as follows:
  
```{r}
#' Orthogonal Projection
#'
#' Projects vector a onto v.
#'
#' @param a   the vector to project
#' @param v   the vector projected onto
#'
#' @returns   a list of two vectors, the orthogonal projection parallel to v named a_parallel, 
#'            and the orthogonal error orthogonal to v called a_perpendicular
orthogonal_projection = function(a, v){
  H=v %*% t(v) / norm_vec(v)^2
  a_parallel=H %*% a
  a_perpendicular=a-a_parallel
  list(a_parallel = a_parallel, a_perpendicular = a_perpendicular)
}
```

#Provide predictions for each of these computations and then run them to make sure you're correct.

```{r}
orthogonal_projection(c(1,2,3,4), c(1,2,3,4))
#prediction: This makes sense.
orthogonal_projection(c(1, 2, 3, 4), c(0, 2, 0, -1))
#prediction: The dot product is zero so the two vectors are perpendicular. The parallel vector should be zero and the perpendicular vector should be itself.
result = orthogonal_projection(c(2, 6, 7, 3), c(1, 3, 5, 7)*37)
t(result$a_parallel) %*% result$a_perpendicular
#prediction:
result$a_parallel + result$a_perpendicular
#prediction: tHIS should be reconstructed the original vector.
result$a_parallel / c(1, 3, 5 ,7)*37
#prediction: The scalar.
```

#Let's use the Boston Housing Data for the following exercises

```{r}
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
p_plus_one = ncol(X)
n = nrow(X)
head(X)
```

#Using your function `orthogonal_projection` orthogonally project onto the column space of X by projecting y on each vector of X individually and adding up the projections and call the sum `yhat_naive`.

```{r}
yhat_naive = rep(0,n)
  for(j in 1:p_plus_one){
    yhat_naive= yhat_naive+orthogonal_projection(y,X[,j])$a_parallel
  }
```

#How much double counting occurred? Measure the magnitude relative to the true LS orthogonal projection.

```{r}
yhat = X %*% solve((t(X) %*% X)) %*% t(X) %*% y
sqrt(sum(yhat_naive^2)) / sqrt(sum(yhat^2))
```

#Is this ratio expected? Why or why not?
  
It is expected to be different from one.
  
#Convert X into V where V has the same column space as X but has orthogonal columns. You can use the function `orthogonal_projection`. This is the Gram-Schmidt orthogonalization algorithm.

```{r}
V = matrix(NA, nrow = n, ncol = p_plus_one)
V[ , 1] = X[ , 1]
for(j in 2:p_plus_one){
  V[,j]= X[,j] 
  for(k in 1:(j-1)){
    V[,j]=V[,j] - orthogonal_projection(X[,j],V[,k])$a_parallel
  }
}

V[,7] %*% V[,9]
```

#Convert V into Q whose columns are the same except normalized

```{r}
Q = matrix(NA, nrow = n, ncol = p_plus_one)
for (j in 1:p_plus_one){
  Q[,j]=V[,j]/ norm_vec(V[,j])
}
```

#Verify Q^T Q is I_{p+1} i.e. Q is an orthonormal matrix.


expect_equal(t(Q) %*% Q, diag(p_plus_one))
expect_equal(Q, Q_from_Rs_builtin)


#Is your Q the same as what results from R's built-in QR-decomposition function?

```{r}
Q_from_Rs_builtin = qr.Q(qr(X))
```

#Is this expected? Why did this happen?

There are infinite orthonormal bases of any column space. The projection will be exactly the same. 

#Project y onto colsp[Q] and verify it is the same as the OLS fit. You may have to use the function `unname` to compare the vectors since they the entries will likely have different names.

```{r}
y_hat = lm(y~X)$fitted.values
expect_equal(c(unname(Q %*% t(Q) %*% y)), unname(y_hat))
```

Project y onto colsp[Q] one by one and verify it sums to be the projection onto the whole space.

```{r}
yhat_naive = rep(0,n)
for(j in 1:p_plus_one){
    yhat_naive = yhat_naive + orthogonal_projection(y, Q[,j])$a_parallel
}
H = Q %*% solve(t(Q) %*% Q) %*% t(Q)
expect_equal(H %*% y, yhat_naive)
```

Split the Boston Housing Data into a training set and a test set where the training set is 80% of the observations. Do so at random.

```{r}
K = 5
n_test = round(n * 1 / K)
n_train = n - n_test
test_ind = sample(1:n, n_test)
train_ind= setdiff(1:n, test_ind)
X_test = X[test_ind,]
X_train = X[train_ind]
Y_test = y[test_ind]
Y_train = y[train_ind]
```

Fit an OLS model. Find the s_e in sample and out of sample. Which one is greater? Note: we are now using s_e and not RMSE since RMSE has the n-(p + 1) in the denominator not n-1 which attempts to de-bias the error estimate by inflating the estimate when overfitting in high p. Again, we're just using `sd(e)`, the sample standard deviation of the residuals.

```{r}
mod = lm(Y_train ~ . + 0, data.frame(X_train))
sd(mod$residuals) 
y_hat = predict(mod, data.frame(X_test))
e = Y_test - y_hat
oos_SE = sd(e)
oos_SE
```

Do these two exercises `Nsim = 1000` times and find the average difference between s_e and ooss_e. 

Nsim = 1000
sum = 0
for (i in 1:Nsim) {
  test_indices = sample(1 : n, n_test)
  train_indices = setdiff(1 : n, test_indices)
  X_train = X[train_indices,]
  y_train = y[train_indices]
  X_test = X[test_indices,]
  y_test = y[test_indices]
  ols_mod = lm(y_train ~ .+0, data.frame(X_train))
  s_e = sd(ols_mod$residuals)
  y_oos = predict(ols_mod, data.frame(X_test))
  residuals = y_test - y_oos
  ooss_e = sd(residuals)
  sum = sum + abs(s_e - ooss_e)
}
avg_diff = sum / Nsim
avg_diff

#We'll now add random junk to the data so that `p_plus_one = n_train` and create a new data matrix `X_with_junk.`

```{r}
X_with_junk = cbind(X, matrix(rnorm(n * (n_train - p_plus_one)), nrow = n))
dim(X)
dim(X_with_junk)
```

Repeat the exercise above measuring the average s_e and ooss_e but this time record these metrics by number of features used. That is, do it for the first column of `X_with_junk` (the intercept column), then do it for the first and second columns, then the first three columns, etc until you do it for all columns of `X_with_junk`. Save these in `s_e_by_p` and `ooss_e_by_p`.

test_indices = sample(1 : n, n_test)
train_indices = setdiff(1 : n, test_indices)
s_e_by_p = rep(NA, ncol(X_with_junk))
ooss_e_by_p = rep(NA, ncol(X_with_junk))
sum_by_p = 0
oos_sum_by_p = 0
Nsim = 100
for (i in 1 : Nsim) {
  for (j in 1 : ncol(X_with_junk)) {
  X_train = X_with_junk[train_indices, 1 : j, drop = FALSE]
  y_train = y[train_indices]
  X_test = X_with_junk[test_indices, 1 : j, drop = FALSE]
  y_test = y[test_indices]
  in_mod = lm(y_train ~ .+0, data.frame(X_train))
  oos_y = predict(in_mod, data.frame(X_test))
  s_e_by_p[j] = sd(in_mod$residuals)
  ooss_e_by_p[j] = sd(y_test - oos_y)
  }
  sum_by_p = sum_by_p + sum(s_e_by_p)
  oos_sum_by_p = oos_sum_by_p + sum(ooss_e_by_p)
}
sum_by_p / (ncol(X_with_junk) * Nsim)
oos_sum_by_p / (ncol(X_with_junk) * Nsim)

#You can graph them here:


pacman::p_load(ggplot2)
ggplot(
  rbind(
    data.frame(s_e = s_e_by_p, p = 1 : n_train, series = "in-sample"),
    data.frame(s_e = ooss_e_by_p, p = 1 : n_train, series = "out-of-sample")
  )) +
  geom_line(aes(x = p, y = s_e, col = series))

 
#Is this shape expected? Explain.

Yes this shape is expected because we added more features. 