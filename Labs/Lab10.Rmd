---
title: "Lab 10"
author: "Amy Butler"
output: pdf_document
date: "NOT DUE"
---
  
Load up the Boston Housing Data and separate into `X` and `y`.

```{r}
pacman::p_load(MASS)
y=Boston$medv
X=Boston[,1:13]
```

Similar to lab 1, write a function that takes a matrix and punches holes (i.e. sets entries equal to `NA`) randomly with an argument `prob_missing`.

```{r}
punch_hole= function(X, prob_missing){
  nr = nrow(X)
  nc = ncol(X)
  M = matrix(rbinom(nr*nc,1,prob_missing),nrow=nr,ncol=nc)
  X[M==1] = NA
  X
}
```

Create a matrix `Xmiss` which is `X` but has missingness with probability of 10% using the previous function.

```{r}
Xmiss = punch_hole(X, 0.10)
Xmiss
```

What type of missing data mechanism created the missingness in `Xmiss`?
  
MCAR
  
Impute using the feature averages to create a matrix `Ximpnaive`.

```{r}
Ximpnaive = Xmiss
Xj_bars = colMeans(Xmiss, na.rm=TRUE)
Xj_bars
for(j in 1:ncol(X)){
  for(i in 1:nrow(X)){
    if(is.na(Xmiss[i,j]))
    Ximpnaive[i,j]=Xj_bars[j]
  }
}
Ximpnaive
```

Use `missForest` to impute the missing entries to create a matrix `XimpMF`.

```{r}
pacman::p_load(missForest)
XimpMF = missForest(data.frame(Xmiss))$ximp
```

What is the s_e of the error for both the naive imputation with feature averages and the intelligent imputation with missForest?
  
```{r}
sd(c((as.matrix(X[is.na(Xmiss)]) - as.matrix(Ximpnaive[is.na(Xmiss)]))))
sd(c((as.matrix(X[is.na(Xmiss)]) - as.matrix(XimpMF[is.na(Xmiss)]))))
```

Create a function that creates missingness in the feature `rm` that is a MAR missing data mechanism.

```{r}
MakeMar = function(x_i,gamma){
  if(x_i$chas==1 & runif(1)<gamma){
    x_i$rm=NA
  }
  x_i
}
```

Create a function that creates missingness in the feature `rm` that is a NMAR missing data mechanism.

```{r}
MakeNMar = function(x_i,gamma){
  if(x_i$rm>6.2 & runif(1)<gamma){
    x_i$rm=NA
  }
  x_i
}
```

Run an OLS model on the diamonds dataset using only the features `carat` and `table`. Print out the coefficients.

```{r}
pacman::p_load(ggplot2)
lm(price~carat+table, diamonds)
```

Interpret the coefficient for `carat`

Lecture 26 Notes

Run a logistic regression probability estimation model on the adult dataset using only the features `age` and `education_num`. Print out the coefficients.


```{r}
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
glm(income~age+education_num,adult, family="binomial")
```


Interpret the coefficient for `education_num`

Lecture 26 notes

Let y = the binary category which is 1 if the income is >50K and 0 if not and x = education_num. Let z = one of the causal variables that influences y directly. Is this an example of causal scenario A, B or C. Explain.

B or C

In a matrix X, generate n = 200 observations each with p = 200000 features which are all realizations from an iid N(0, 1) r.v. Then generate responses y, a vector of length n also from an iid N(0, 1) r.v.

```{r}
n=200
p=200000
X = matrix(rnorm(n*p), nrow=n, ncol=p)
y = rnorm(n)
```

Scan through each of the 200000 features looking for the maximum R^2 between x_j and y among only the first 100 observations. Plot the x_j and y that has the highest R^2 for the first 30 observations.

```{r}
pacman::p_load(ggplot2)
Rsqs = array(NA, dim=p)
for (j in 1:p){
  Rsqs[j]=cor(X[1:30],y[1:30])^2
}
j_star=which.max(Rsqs)
j_star
Rsqs[j_star]
ggplot(data.frame(x=X[1:30, j_star], y=y[1:30]))+
  aes(x=x, y=y)+
  geom_point()
```

Now plot this x_j and y for all 200 observations.

```{r}
ggplot(data.frame(x=X[, j_star], y=y))+
  aes(x=x, y=y)+
  geom_point()
```

Is this an example of a "spurious correlation"? Yes/no and explain.

Yes, correlated by chance.

Run the following code to create a dataset but don't read it:

```{r}
rm(list = ls())
set.seed(1)
n = 200
salary_data = rbind(
  data.frame(
    is_male = rep(1, n / 2),
    height_in_inches = rnorm(n / 2, 70, 3),
    salary_in_thou = rnorm(n / 2, 60, 15)
  ),
  data.frame(
    is_male = rep(0, n / 2),
    height_in_inches = rnorm(n / 2, 64, 3),
    salary_in_thou = rnorm(n / 2, 50, 15)
  )
)
```

Using the `salary_data` data frame, run an OLS model predicting `salary_in_thou` using `height_in_inches`.

```{r}
summary(lm(salary_in_thou ~ height_in_inches, salary_data))
```

Interpret the coefficient of `height_in_inches`.
 
Lecture 26 Notes

Plot `salary_in_thou` vs `height_in_inches`.

```{r}
ggplot(salary_data)+
  aes(x=height_in_inches, y=salary_in_thou)+
  geom_point()
```

Now run an OLS model predicting `salary_in_thou` using both `height_in_inches` and `is_male`.

```{r}
summary(lm(salary_in_thou ~ height_in_inches + is_male, salary_data))
```

Interpret the coefficient of `height_in_inches`.

#TO-DO

Although we didn't discuss this in class, the *'s in the summary of a linear model indicates there is evidence that this OLS slope coefficient is nonzero. In the first model, there was evidence that the OLS slope coefficient for `height_in_inches` was nonzero but in the second model there is no longer any evidence that the OLS slope coefficient for `height_in_inches`is nonzero. This may indicate that `is_male` is what type of variable? 
 
The lurking variable.

Of the three causal scenarios we discussed in class (A, B and C), what is the likely scenario here?
 
B

Are we sure that `is_male` is a causal variable with respect to the phenomenon `salary_in_thou`? Yes/no and explain.


In the `diamonds` data, consider the OLS model where the features are all second-order interactions. Use a cross-validated lasso (via the `glmnet.cv` function in the `glmnet` package) to select variables that are useful in predicting the dimaonds' prices. Print out a list of the selected variables. If this takes too long, subsample the data so there is n=2000 observations.

```{r}
rm(list = ls())
pacman::p_load(glmnet)
#TO-DO
```

In the `adult` data, consider the logistic regression model of all second-order interactions. Use a cross-validated lasso (via the `glmnet.cv` function in the `glmnet` package) to select variables that are useful in predicting the binary income level. We never discussed lasso for logistic regression, but it is the same as regular logistic regression where you minimize the likelihood but now add the regularization penalty to the optimization problem. This is all handled for us by merely passing the `family = "binomial"` argument into the `glmnet.cv` function. Print out a list of the selected variables. If this takes too long, subsample the data so there is n=2000 observations.

```{r}
rm(list = ls())
#TO-DO
```

Returning to the diamonds dataset, leave a 10% holdout and compare the oos performance of the linear model of all second-order interactions among the following three algorithms: a cross-validated ridge, a cross-validated lasso and a cross-validated elastic net (where alpha = 1/2).

```{r}
rm(list = ls())
#TO-DO
```